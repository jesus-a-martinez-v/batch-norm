{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando Redes Neuronales Muy Profundas Usando Normalización por Lotes en TensorFlow\n",
    "\n",
    "La normalización por lotes realmente reporta dividendos cuando entrenamos redes _muy_ profundas.\n",
    "\n",
    "Nuestras meta en este notebook es demostrar esta propiedad. Para tal fin, construiremos dos redes: Una con normalización y otra sin ella para reconocer números escritos a mano alzada provenientes de la base de datos MNIST.\n",
    "\n",
    "**AVISO**: Estas arquitecturas NO son las mejores para MNIST. Son demasiado complicadas, y aunque una red más sencilla produciría mejores resultados, a propósito la hicimos así de complejas para poner en evidencia la utilidad de la normalización por lotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminares\n",
    "\n",
    "Carguemos los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2bac6a393a04>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red de Alto Nivel\n",
    "\n",
    "Esta versión de la red utiliza las funciones auxiliares en el paquete `tf.layers` que son de muy alto nivel (al menos para los estándares de TensorFlow). Luego, reharemos el trabajo usando un API de más bajo nivel.\n",
    "\n",
    "Empecemos construyendo una red **sin** normalización por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units):\n",
    "    return tf.layers.dense(previous_layer, number_of_units, activation=tf.nn.relu)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    return tf.layers.conv2d(previous_layer, layer_depth * 4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, labels: batch_ys})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]]})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.690942108631134, Validation accuracy: 0.10700000077486038\n",
      "Batch: 25, Training loss: 0.3634071350097656, Training accuracy: 0.0625\n",
      "Batch: 50, Training loss: 0.3253903090953827, Training accuracy: 0.1875\n",
      "Batch: 75, Training loss: 0.33056578040122986, Training accuracy: 0.03125\n",
      "Batch: 100, Validation loss: 0.32672563195228577, Validation accuracy: 0.10999999940395355\n",
      "Batch: 125, Training loss: 0.3267457187175751, Training accuracy: 0.09375\n",
      "Batch: 150, Training loss: 0.32452020049095154, Training accuracy: 0.15625\n",
      "Batch: 175, Training loss: 0.32730501890182495, Training accuracy: 0.0625\n",
      "Batch: 200, Validation loss: 0.3261047601699829, Validation accuracy: 0.09759999811649323\n",
      "Batch: 225, Training loss: 0.3210393786430359, Training accuracy: 0.21875\n",
      "Batch: 250, Training loss: 0.32537251710891724, Training accuracy: 0.125\n",
      "Batch: 275, Training loss: 0.3301261365413666, Training accuracy: 0.0625\n",
      "Batch: 300, Validation loss: 0.3255506157875061, Validation accuracy: 0.10019999742507935\n",
      "Batch: 325, Training loss: 0.3223673105239868, Training accuracy: 0.09375\n",
      "Batch: 350, Training loss: 0.3231832981109619, Training accuracy: 0.1875\n",
      "Batch: 375, Training loss: 0.325949490070343, Training accuracy: 0.1875\n",
      "Batch: 400, Validation loss: 0.3256467282772064, Validation accuracy: 0.09759999811649323\n",
      "Batch: 425, Training loss: 0.32460445165634155, Training accuracy: 0.125\n",
      "Batch: 450, Training loss: 0.3281840980052948, Training accuracy: 0.0\n",
      "Batch: 475, Training loss: 0.326241672039032, Training accuracy: 0.125\n",
      "Batch: 500, Validation loss: 0.326262503862381, Validation accuracy: 0.09860000014305115\n",
      "Batch: 525, Training loss: 0.3241123557090759, Training accuracy: 0.1875\n",
      "Batch: 550, Training loss: 0.32717013359069824, Training accuracy: 0.09375\n",
      "Batch: 575, Training loss: 0.32371875643730164, Training accuracy: 0.0625\n",
      "Batch: 600, Validation loss: 0.3250825107097626, Validation accuracy: 0.10019999742507935\n",
      "Batch: 625, Training loss: 0.32254984974861145, Training accuracy: 0.09375\n",
      "Batch: 650, Training loss: 0.3276955485343933, Training accuracy: 0.0\n",
      "Batch: 675, Training loss: 0.3281225264072418, Training accuracy: 0.15625\n",
      "Batch: 700, Validation loss: 0.3260502219200134, Validation accuracy: 0.09860000014305115\n",
      "Batch: 725, Training loss: 0.3250196874141693, Training accuracy: 0.0625\n",
      "Batch: 750, Training loss: 0.32343125343322754, Training accuracy: 0.21875\n",
      "Batch: 775, Training loss: 0.3249794840812683, Training accuracy: 0.03125\n",
      "Final validaction accuracy: 0.0989999994635582\n",
      "Final test accuracy: 0.10090000182390213\n",
      "Accuracy on 100 samples: 0.11\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la profundidad de esta red, le tomará **muchísimo tiempo** aprender algo. De hecho, después de 800 lotes, apenas alcanza un 10% de _accuracy_. Eso no está bien. Veamos cómo le va a la versión normalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units, is_training):\n",
    "    layer = tf.layers.dense(previous_layer, number_of_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(previous_layer, layer_depth * 4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    return tf.nn.relu(conv_layer)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i, is_training=is_training)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100, is_training)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, \n",
    "                                              labels: batch_ys, \n",
    "                                              is_training: True})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, \n",
    "                                                                     labels: batch_ys,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]],\n",
    "                                                  is_training: False})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenemos nuevamente la red, pero esta vez aplicando normalización por lotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.6910560727119446, Validation accuracy: 0.0989999994635582\n",
      "Batch: 25, Training loss: 0.5869064927101135, Training accuracy: 0.0625\n",
      "Batch: 50, Training loss: 0.4755508303642273, Training accuracy: 0.09375\n",
      "Batch: 75, Training loss: 0.4092184603214264, Training accuracy: 0.03125\n",
      "Batch: 100, Validation loss: 0.369232177734375, Validation accuracy: 0.0989999994635582\n",
      "Batch: 125, Training loss: 0.3442865014076233, Training accuracy: 0.0625\n",
      "Batch: 150, Training loss: 0.3285561203956604, Training accuracy: 0.15625\n",
      "Batch: 175, Training loss: 0.31170812249183655, Training accuracy: 0.25\n",
      "Batch: 200, Validation loss: 0.3115042448043823, Validation accuracy: 0.22280000150203705\n",
      "Batch: 225, Training loss: 0.2478412389755249, Training accuracy: 0.5\n",
      "Batch: 250, Training loss: 0.1931648701429367, Training accuracy: 0.6875\n",
      "Batch: 275, Training loss: 0.10294921696186066, Training accuracy: 0.78125\n",
      "Batch: 300, Validation loss: 0.3323472738265991, Validation accuracy: 0.46779999136924744\n",
      "Batch: 325, Training loss: 0.14310626685619354, Training accuracy: 0.71875\n",
      "Batch: 350, Training loss: 0.08733728528022766, Training accuracy: 0.875\n",
      "Batch: 375, Training loss: 0.14338216185569763, Training accuracy: 0.8125\n",
      "Batch: 400, Validation loss: 0.061416324228048325, Validation accuracy: 0.8938000202178955\n",
      "Batch: 425, Training loss: 0.007792363408952951, Training accuracy: 0.96875\n",
      "Batch: 450, Training loss: 0.052779246121644974, Training accuracy: 0.90625\n",
      "Batch: 475, Training loss: 0.03688869625329971, Training accuracy: 0.90625\n",
      "Batch: 500, Validation loss: 0.047691840678453445, Validation accuracy: 0.9240000247955322\n",
      "Batch: 525, Training loss: 0.07944413274526596, Training accuracy: 0.90625\n",
      "Batch: 550, Training loss: 0.012870458886027336, Training accuracy: 0.96875\n",
      "Batch: 575, Training loss: 0.005150252487510443, Training accuracy: 1.0\n",
      "Batch: 600, Validation loss: 0.07129857689142227, Validation accuracy: 0.900600016117096\n",
      "Batch: 625, Training loss: 0.033011648803949356, Training accuracy: 0.9375\n",
      "Batch: 650, Training loss: 0.059091854840517044, Training accuracy: 0.9375\n",
      "Batch: 675, Training loss: 0.06890328228473663, Training accuracy: 0.90625\n",
      "Batch: 700, Validation loss: 0.0529462993144989, Validation accuracy: 0.9300000071525574\n",
      "Batch: 725, Training loss: 0.06064540892839432, Training accuracy: 0.9375\n",
      "Batch: 750, Training loss: 0.06045369431376457, Training accuracy: 0.96875\n",
      "Batch: 775, Training loss: 0.013325333595275879, Training accuracy: 0.96875\n",
      "Final validaction accuracy: 0.9584000110626221\n",
      "Final test accuracy: 0.9611999988555908\n",
      "Accuracy on 100 samples: 0.98\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Increíble! En el mismo número de lotes esta red consiguió un _accuracy_ de prueba bastante decende de 96.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Red de Bajo Nivel\n",
    "\n",
    "Esta versión de la red utiliza las funciones auxiliares en `tf.nn` que son de un nivel considerablemente más bajo que auquellas en `tf.layers`. Esto es útil porque, en ocasiones, queremos tener más control sobre cómo implementar un funcionalidad.\n",
    "\n",
    "**NOTA**: Para poder entender las implementaciones en la próxima celda, es una buena idea leer primero el [paper original](https://arxiv.org/abs/1502.03167)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units, is_training):\n",
    "    layer = tf.layers.dense(previous_layer, number_of_units, use_bias=False, activation=None)\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([number_of_units]))\n",
    "    beta = tf.Variable(tf.zeros([number_of_units]))\n",
    "    \n",
    "    population_mean = tf.Variable(tf.zeros([number_of_units]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([number_of_units]), trainable=False)\n",
    "    \n",
    "    epsilon = 0.001\n",
    "    \n",
    "    def batch_normalization_training():\n",
    "        batch_mean, batch_variance =  tf.nn.moments(layer, [0])\n",
    "        \n",
    "        decay = 0.99\n",
    "        \n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_normalization_inference():\n",
    "        return tf.nn.batch_normalization(layer, population_mean, population_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_normalization_training, batch_normalization_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    input_channels = previous_layer.get_shape().as_list()[3]\n",
    "    output_channels = layer_depth * 4\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal((3, 3, input_channels, output_channels), stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(previous_layer, weights, strides=(1, strides, strides, 1), padding='SAME')\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([output_channels]))\n",
    "    beta = tf.Variable(tf.zeros([output_channels]))\n",
    "    \n",
    "    population_mean = tf.Variable(tf.zeros([output_channels]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([output_channels]), trainable=False)\n",
    "    \n",
    "    epsilon = 0.001\n",
    "    \n",
    "    def batch_normalization_training():\n",
    "        batch_mean, batch_variance =  tf.nn.moments(layer, [0, 1, 2], keep_dims=False)\n",
    "        \n",
    "        decay = 0.99\n",
    "        \n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_normalization_inference():\n",
    "        return tf.nn.batch_normalization(layer, population_mean, population_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_normalization_training, batch_normalization_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i, is_training=is_training)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100, is_training)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, \n",
    "                                              labels: batch_ys, \n",
    "                                              is_training: True})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, \n",
    "                                                                     labels: batch_ys,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]],\n",
    "                                                  is_training: False})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, entrenemos la red usando las funciones de bajo nivel que acabamos de implementar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.6910102963447571, Validation accuracy: 0.11259999871253967\n",
      "Batch: 25, Training loss: 0.5824524164199829, Training accuracy: 0.21875\n",
      "Batch: 50, Training loss: 0.46968093514442444, Training accuracy: 0.09375\n",
      "Batch: 75, Training loss: 0.4036182761192322, Training accuracy: 0.0625\n",
      "Batch: 100, Validation loss: 0.36633265018463135, Validation accuracy: 0.10019999742507935\n",
      "Batch: 125, Training loss: 0.34632745385169983, Training accuracy: 0.125\n",
      "Batch: 150, Training loss: 0.3398407995700836, Training accuracy: 0.09375\n",
      "Batch: 175, Training loss: 0.3682170510292053, Training accuracy: 0.0625\n",
      "Batch: 200, Validation loss: 0.39046332240104675, Validation accuracy: 0.0868000015616417\n",
      "Batch: 225, Training loss: 0.4842556416988373, Training accuracy: 0.03125\n",
      "Batch: 250, Training loss: 0.5430601239204407, Training accuracy: 0.09375\n",
      "Batch: 275, Training loss: 0.5405094027519226, Training accuracy: 0.125\n",
      "Batch: 300, Validation loss: 0.6523125171661377, Validation accuracy: 0.0868000015616417\n",
      "Batch: 325, Training loss: 0.7546620965003967, Training accuracy: 0.0625\n",
      "Batch: 350, Training loss: 0.64732825756073, Training accuracy: 0.15625\n",
      "Batch: 375, Training loss: 0.5014845728874207, Training accuracy: 0.3125\n",
      "Batch: 400, Validation loss: 0.8380928039550781, Validation accuracy: 0.1518000066280365\n",
      "Batch: 425, Training loss: 0.6016968488693237, Training accuracy: 0.375\n",
      "Batch: 450, Training loss: 0.1507897675037384, Training accuracy: 0.71875\n",
      "Batch: 475, Training loss: 0.3905387222766876, Training accuracy: 0.46875\n",
      "Batch: 500, Validation loss: 0.16015538573265076, Validation accuracy: 0.6991999745368958\n",
      "Batch: 525, Training loss: 0.3733879327774048, Training accuracy: 0.4375\n",
      "Batch: 550, Training loss: 0.191448375582695, Training accuracy: 0.75\n",
      "Batch: 575, Training loss: 0.10763140022754669, Training accuracy: 0.8125\n",
      "Batch: 600, Validation loss: 0.09283912926912308, Validation accuracy: 0.8682000041007996\n",
      "Batch: 625, Training loss: 0.029856840148568153, Training accuracy: 0.96875\n",
      "Batch: 650, Training loss: 0.027061522006988525, Training accuracy: 0.9375\n",
      "Batch: 675, Training loss: 0.24060550332069397, Training accuracy: 0.8125\n",
      "Batch: 700, Validation loss: 0.06847918033599854, Validation accuracy: 0.9333999752998352\n",
      "Batch: 725, Training loss: 0.000981668708845973, Training accuracy: 1.0\n",
      "Batch: 750, Training loss: 0.012879764661192894, Training accuracy: 1.0\n",
      "Batch: 775, Training loss: 0.020740356296300888, Training accuracy: 0.9375\n",
      "Final validaction accuracy: 0.9276000261306763\n",
      "Final test accuracy: 0.9235000014305115\n",
      "Accuracy on 100 samples: 0.96\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Grandioso! Funciona. La normalización por lotes es una herramienta poderosa, como hemos podido ver en este notebook. La mayoría de las veces podemos usar las funciones de `tf.layers` con confianza, pero también vale la pena saber cómo trabajar con el API de bajo nivel, dado que nos otorga mayor control sobre nuestras soluciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
