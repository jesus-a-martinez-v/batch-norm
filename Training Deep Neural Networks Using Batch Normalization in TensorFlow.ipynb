{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Deep Neural Networks Using Batch Normalization in TensorFlow\n",
    "\n",
    "Batch normalization really pays off when we train _really_ deep neural networks. \n",
    "\n",
    "Our goal in this notebook is to demonstrate this property. For that matter, we'll build two deep neural networks (one with and one without batch normalization) to recognize hand-written numbers from the MNIST database.\n",
    "\n",
    "**DISCLAIMER**: These architectures are NOT the best for the MNIST dataset. They're too complex, and while a simpler network would produce better results, we made it this convoluted on purpose to demonstrate the usefulness of batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-2bac6a393a04>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level Network\n",
    "\n",
    "This version of the network uses the helper functions in `tf.layers` which are very high level (at least for TensorFlow standards). Later we'll redo all the work you'll encounter in the following cells using a lower level API.\n",
    "\n",
    "Let's start by building a network **without** batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units):\n",
    "    return tf.layers.dense(previous_layer, number_of_units, activation=tf.nn.relu)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    return tf.layers.conv2d(previous_layer, layer_depth * 4, 3, strides, 'same', activation=tf.nn.relu)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, labels: batch_ys})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, labels: batch_ys})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]]})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.690942108631134, Validation accuracy: 0.10700000077486038\n",
      "Batch: 25, Training loss: 0.3634071350097656, Training accuracy: 0.0625\n",
      "Batch: 50, Training loss: 0.3253903090953827, Training accuracy: 0.1875\n",
      "Batch: 75, Training loss: 0.33056578040122986, Training accuracy: 0.03125\n",
      "Batch: 100, Validation loss: 0.32672563195228577, Validation accuracy: 0.10999999940395355\n",
      "Batch: 125, Training loss: 0.3267457187175751, Training accuracy: 0.09375\n",
      "Batch: 150, Training loss: 0.32452020049095154, Training accuracy: 0.15625\n",
      "Batch: 175, Training loss: 0.32730501890182495, Training accuracy: 0.0625\n",
      "Batch: 200, Validation loss: 0.3261047601699829, Validation accuracy: 0.09759999811649323\n",
      "Batch: 225, Training loss: 0.3210393786430359, Training accuracy: 0.21875\n",
      "Batch: 250, Training loss: 0.32537251710891724, Training accuracy: 0.125\n",
      "Batch: 275, Training loss: 0.3301261365413666, Training accuracy: 0.0625\n",
      "Batch: 300, Validation loss: 0.3255506157875061, Validation accuracy: 0.10019999742507935\n",
      "Batch: 325, Training loss: 0.3223673105239868, Training accuracy: 0.09375\n",
      "Batch: 350, Training loss: 0.3231832981109619, Training accuracy: 0.1875\n",
      "Batch: 375, Training loss: 0.325949490070343, Training accuracy: 0.1875\n",
      "Batch: 400, Validation loss: 0.3256467282772064, Validation accuracy: 0.09759999811649323\n",
      "Batch: 425, Training loss: 0.32460445165634155, Training accuracy: 0.125\n",
      "Batch: 450, Training loss: 0.3281840980052948, Training accuracy: 0.0\n",
      "Batch: 475, Training loss: 0.326241672039032, Training accuracy: 0.125\n",
      "Batch: 500, Validation loss: 0.326262503862381, Validation accuracy: 0.09860000014305115\n",
      "Batch: 525, Training loss: 0.3241123557090759, Training accuracy: 0.1875\n",
      "Batch: 550, Training loss: 0.32717013359069824, Training accuracy: 0.09375\n",
      "Batch: 575, Training loss: 0.32371875643730164, Training accuracy: 0.0625\n",
      "Batch: 600, Validation loss: 0.3250825107097626, Validation accuracy: 0.10019999742507935\n",
      "Batch: 625, Training loss: 0.32254984974861145, Training accuracy: 0.09375\n",
      "Batch: 650, Training loss: 0.3276955485343933, Training accuracy: 0.0\n",
      "Batch: 675, Training loss: 0.3281225264072418, Training accuracy: 0.15625\n",
      "Batch: 700, Validation loss: 0.3260502219200134, Validation accuracy: 0.09860000014305115\n",
      "Batch: 725, Training loss: 0.3250196874141693, Training accuracy: 0.0625\n",
      "Batch: 750, Training loss: 0.32343125343322754, Training accuracy: 0.21875\n",
      "Batch: 775, Training loss: 0.3249794840812683, Training accuracy: 0.03125\n",
      "Final validaction accuracy: 0.0989999994635582\n",
      "Final test accuracy: 0.10090000182390213\n",
      "Accuracy on 100 samples: 0.11\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the depth of the network, it will take a **really long time** to learn anything. In fact, after 800 batches, it only reaches 10% accuracy. That's not good. Let's see how it goes **with** batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units, is_training):\n",
    "    layer = tf.layers.dense(previous_layer, number_of_units, use_bias=False, activation=None)\n",
    "    layer = tf.layers.batch_normalization(layer, training=is_training)\n",
    "    return tf.nn.relu(layer)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    conv_layer = tf.layers.conv2d(previous_layer, layer_depth * 4, 3, strides, 'same', use_bias=False, activation=None)\n",
    "    conv_layer = tf.layers.batch_normalization(conv_layer, training=is_training)\n",
    "    return tf.nn.relu(conv_layer)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i, is_training=is_training)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100, is_training)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, \n",
    "                                              labels: batch_ys, \n",
    "                                              is_training: True})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, \n",
    "                                                                     labels: batch_ys,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]],\n",
    "                                                  is_training: False})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network again, but this time using batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.6910560727119446, Validation accuracy: 0.0989999994635582\n",
      "Batch: 25, Training loss: 0.5869064927101135, Training accuracy: 0.0625\n",
      "Batch: 50, Training loss: 0.4755508303642273, Training accuracy: 0.09375\n",
      "Batch: 75, Training loss: 0.4092184603214264, Training accuracy: 0.03125\n",
      "Batch: 100, Validation loss: 0.369232177734375, Validation accuracy: 0.0989999994635582\n",
      "Batch: 125, Training loss: 0.3442865014076233, Training accuracy: 0.0625\n",
      "Batch: 150, Training loss: 0.3285561203956604, Training accuracy: 0.15625\n",
      "Batch: 175, Training loss: 0.31170812249183655, Training accuracy: 0.25\n",
      "Batch: 200, Validation loss: 0.3115042448043823, Validation accuracy: 0.22280000150203705\n",
      "Batch: 225, Training loss: 0.2478412389755249, Training accuracy: 0.5\n",
      "Batch: 250, Training loss: 0.1931648701429367, Training accuracy: 0.6875\n",
      "Batch: 275, Training loss: 0.10294921696186066, Training accuracy: 0.78125\n",
      "Batch: 300, Validation loss: 0.3323472738265991, Validation accuracy: 0.46779999136924744\n",
      "Batch: 325, Training loss: 0.14310626685619354, Training accuracy: 0.71875\n",
      "Batch: 350, Training loss: 0.08733728528022766, Training accuracy: 0.875\n",
      "Batch: 375, Training loss: 0.14338216185569763, Training accuracy: 0.8125\n",
      "Batch: 400, Validation loss: 0.061416324228048325, Validation accuracy: 0.8938000202178955\n",
      "Batch: 425, Training loss: 0.007792363408952951, Training accuracy: 0.96875\n",
      "Batch: 450, Training loss: 0.052779246121644974, Training accuracy: 0.90625\n",
      "Batch: 475, Training loss: 0.03688869625329971, Training accuracy: 0.90625\n",
      "Batch: 500, Validation loss: 0.047691840678453445, Validation accuracy: 0.9240000247955322\n",
      "Batch: 525, Training loss: 0.07944413274526596, Training accuracy: 0.90625\n",
      "Batch: 550, Training loss: 0.012870458886027336, Training accuracy: 0.96875\n",
      "Batch: 575, Training loss: 0.005150252487510443, Training accuracy: 1.0\n",
      "Batch: 600, Validation loss: 0.07129857689142227, Validation accuracy: 0.900600016117096\n",
      "Batch: 625, Training loss: 0.033011648803949356, Training accuracy: 0.9375\n",
      "Batch: 650, Training loss: 0.059091854840517044, Training accuracy: 0.9375\n",
      "Batch: 675, Training loss: 0.06890328228473663, Training accuracy: 0.90625\n",
      "Batch: 700, Validation loss: 0.0529462993144989, Validation accuracy: 0.9300000071525574\n",
      "Batch: 725, Training loss: 0.06064540892839432, Training accuracy: 0.9375\n",
      "Batch: 750, Training loss: 0.06045369431376457, Training accuracy: 0.96875\n",
      "Batch: 775, Training loss: 0.013325333595275879, Training accuracy: 0.96875\n",
      "Final validaction accuracy: 0.9584000110626221\n",
      "Final test accuracy: 0.9611999988555908\n",
      "Accuracy on 100 samples: 0.98\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Amazing! In the same number of batches, this new network achieved a decent 96.1% accuracy on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Level Network\n",
    "\n",
    "This version of the network uses the helper functions in `tf.nn` which are at a lower level than those in `tf.layers` package. This is useful because, some times, we want to have more control on how to implement a feature.\n",
    "\n",
    "**NOTE**: In order to understand the implementations details present in the following cell, it's a good idea to first [read the original paper](https://arxiv.org/abs/1502.03167)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected(previous_layer, number_of_units, is_training):\n",
    "    layer = tf.layers.dense(previous_layer, number_of_units, use_bias=False, activation=None)\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([number_of_units]))\n",
    "    beta = tf.Variable(tf.zeros([number_of_units]))\n",
    "    \n",
    "    population_mean = tf.Variable(tf.zeros([number_of_units]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([number_of_units]), trainable=False)\n",
    "    \n",
    "    epsilon = 0.001\n",
    "    \n",
    "    def batch_normalization_training():\n",
    "        batch_mean, batch_variance =  tf.nn.moments(layer, [0])\n",
    "        \n",
    "        decay = 0.99\n",
    "        \n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_normalization_inference():\n",
    "        return tf.nn.batch_normalization(layer, population_mean, population_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_normalization_training, batch_normalization_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "def conv_layer(previous_layer, layer_depth, is_training):\n",
    "    strides = 2 if layer_depth % 3 == 0 else 1\n",
    "    \n",
    "    input_channels = previous_layer.get_shape().as_list()[3]\n",
    "    output_channels = layer_depth * 4\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal((3, 3, input_channels, output_channels), stddev=0.05))\n",
    "    \n",
    "    layer = tf.nn.conv2d(previous_layer, weights, strides=(1, strides, strides, 1), padding='SAME')\n",
    "    \n",
    "    gamma = tf.Variable(tf.ones([output_channels]))\n",
    "    beta = tf.Variable(tf.zeros([output_channels]))\n",
    "    \n",
    "    population_mean = tf.Variable(tf.zeros([output_channels]), trainable=False)\n",
    "    population_variance = tf.Variable(tf.ones([output_channels]), trainable=False)\n",
    "    \n",
    "    epsilon = 0.001\n",
    "    \n",
    "    def batch_normalization_training():\n",
    "        batch_mean, batch_variance =  tf.nn.moments(layer, [0, 1, 2], keep_dims=False)\n",
    "        \n",
    "        decay = 0.99\n",
    "        \n",
    "        train_mean = tf.assign(population_mean, population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_variance = tf.assign(population_variance, population_variance * decay + batch_variance * (1 - decay))\n",
    "        \n",
    "        with tf.control_dependencies([train_mean, train_variance]):\n",
    "            return tf.nn.batch_normalization(layer, batch_mean, batch_variance, beta, gamma, epsilon)\n",
    "        \n",
    "    def batch_normalization_inference():\n",
    "        return tf.nn.batch_normalization(layer, population_mean, population_variance, beta, gamma, epsilon)\n",
    "    \n",
    "    batch_normalized_output = tf.cond(is_training, batch_normalization_training, batch_normalization_inference)\n",
    "    return tf.nn.relu(batch_normalized_output)\n",
    "\n",
    "def train(number_of_batches, batch_size, learning_rate):\n",
    "    inputs = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "    labels = tf.placeholder(tf.float32, (None, 10))\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    network = inputs\n",
    "    for i in range(1, 20):\n",
    "        network = conv_layer(network, layer_depth=i, is_training=is_training)\n",
    "        \n",
    "    # Flatten\n",
    "    original_shape = network.get_shape().as_list()\n",
    "    network = tf.reshape(network, shape=(-1, original_shape[1] * original_shape[2] * original_shape[3]))\n",
    "    \n",
    "    network = fully_connected(network, 100, is_training)\n",
    "    \n",
    "    logits = tf.layers.dense(network, 10)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    train_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # Time to train\n",
    "    with tf.Session() as s:\n",
    "        s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for i in range(number_of_batches):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            s.run(train_optimizer, feed_dict={inputs: batch_xs, \n",
    "                                              labels: batch_ys, \n",
    "                                              is_training: True})\n",
    "            \n",
    "            # Check validation or training loss and accuracy\n",
    "            if i % 100 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: mnist.validation.images, \n",
    "                                                                     labels: mnist.validation.labels,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Validation loss: {loss}, Validation accuracy: {acc}')\n",
    "            elif i % 25 == 0:\n",
    "                loss, acc = s.run([model_loss, accuracy], feed_dict={inputs: batch_xs, \n",
    "                                                                     labels: batch_ys,\n",
    "                                                                     is_training: False})\n",
    "                print(f'Batch: {i}, Training loss: {loss}, Training accuracy: {acc}')\n",
    "              \n",
    "        # Final accuracy for both validation and test sets\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.validation.images, \n",
    "                                         labels: mnist.validation.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final validaction accuracy: {acc}')\n",
    "        acc = s.run(accuracy, feed_dict={inputs: mnist.test.images, \n",
    "                                         labels: mnist.test.labels, \n",
    "                                         is_training: False})\n",
    "        print(f'Final test accuracy: {acc}')\n",
    "        \n",
    "        # Score the first 100 test images.\n",
    "        correct = 0.0\n",
    "        for i in range(100):\n",
    "            correct += s.run(accuracy, feed_dict={inputs: [mnist.test.images[i]], \n",
    "                                                  labels: [mnist.test.labels[i]],\n",
    "                                                  is_training: False})\n",
    "            \n",
    "        print(f'Accuracy on 100 samples: {correct/100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's the network using the lower level helper functions we just implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Validation loss: 0.6910102963447571, Validation accuracy: 0.11259999871253967\n",
      "Batch: 25, Training loss: 0.5824524164199829, Training accuracy: 0.21875\n",
      "Batch: 50, Training loss: 0.46968093514442444, Training accuracy: 0.09375\n",
      "Batch: 75, Training loss: 0.4036182761192322, Training accuracy: 0.0625\n",
      "Batch: 100, Validation loss: 0.36633265018463135, Validation accuracy: 0.10019999742507935\n",
      "Batch: 125, Training loss: 0.34632745385169983, Training accuracy: 0.125\n",
      "Batch: 150, Training loss: 0.3398407995700836, Training accuracy: 0.09375\n",
      "Batch: 175, Training loss: 0.3682170510292053, Training accuracy: 0.0625\n",
      "Batch: 200, Validation loss: 0.39046332240104675, Validation accuracy: 0.0868000015616417\n",
      "Batch: 225, Training loss: 0.4842556416988373, Training accuracy: 0.03125\n",
      "Batch: 250, Training loss: 0.5430601239204407, Training accuracy: 0.09375\n",
      "Batch: 275, Training loss: 0.5405094027519226, Training accuracy: 0.125\n",
      "Batch: 300, Validation loss: 0.6523125171661377, Validation accuracy: 0.0868000015616417\n",
      "Batch: 325, Training loss: 0.7546620965003967, Training accuracy: 0.0625\n",
      "Batch: 350, Training loss: 0.64732825756073, Training accuracy: 0.15625\n",
      "Batch: 375, Training loss: 0.5014845728874207, Training accuracy: 0.3125\n",
      "Batch: 400, Validation loss: 0.8380928039550781, Validation accuracy: 0.1518000066280365\n",
      "Batch: 425, Training loss: 0.6016968488693237, Training accuracy: 0.375\n",
      "Batch: 450, Training loss: 0.1507897675037384, Training accuracy: 0.71875\n",
      "Batch: 475, Training loss: 0.3905387222766876, Training accuracy: 0.46875\n",
      "Batch: 500, Validation loss: 0.16015538573265076, Validation accuracy: 0.6991999745368958\n",
      "Batch: 525, Training loss: 0.3733879327774048, Training accuracy: 0.4375\n",
      "Batch: 550, Training loss: 0.191448375582695, Training accuracy: 0.75\n",
      "Batch: 575, Training loss: 0.10763140022754669, Training accuracy: 0.8125\n",
      "Batch: 600, Validation loss: 0.09283912926912308, Validation accuracy: 0.8682000041007996\n",
      "Batch: 625, Training loss: 0.029856840148568153, Training accuracy: 0.96875\n",
      "Batch: 650, Training loss: 0.027061522006988525, Training accuracy: 0.9375\n",
      "Batch: 675, Training loss: 0.24060550332069397, Training accuracy: 0.8125\n",
      "Batch: 700, Validation loss: 0.06847918033599854, Validation accuracy: 0.9333999752998352\n",
      "Batch: 725, Training loss: 0.000981668708845973, Training accuracy: 1.0\n",
      "Batch: 750, Training loss: 0.012879764661192894, Training accuracy: 1.0\n",
      "Batch: 775, Training loss: 0.020740356296300888, Training accuracy: 0.9375\n",
      "Final validaction accuracy: 0.9276000261306763\n",
      "Final test accuracy: 0.9235000014305115\n",
      "Accuracy on 100 samples: 0.96\n"
     ]
    }
   ],
   "source": [
    "NUM_BATCHES = 800\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.002\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default():\n",
    "    train(NUM_BATCHES, BATCH_SIZE, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It works. Batch normalization is a powerful tool, as we've seen in this notebook. Most of the time we can safely use the higher level functions in the `tf.layers` package, but it's also worth knowing how to work with the low level functions, as it gives us more control of our solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
